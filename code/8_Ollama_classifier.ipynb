{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9bffc3-ad16-488d-91e7-8e46c746acce",
   "metadata": {},
   "source": [
    "# LLMs desplegados localemente\n",
    "## Data Mining - Doctorado UDP 2025\n",
    "**Bastián González-Bustamante** \\\n",
    "Noviembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a53cf9-7eb9-475c-92b0-7986e19bf11d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b04b4a-1bb8-45e8-8f54-1c9ae0137b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_obs</th>\n",
       "      <th>coder_1</th>\n",
       "      <th>coder_2</th>\n",
       "      <th>consensus</th>\n",
       "      <th>sec_create_1</th>\n",
       "      <th>sec_create_2</th>\n",
       "      <th>sec_review_1</th>\n",
       "      <th>sec_review_2</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>THREAT</th>\n",
       "      <th>date</th>\n",
       "      <th>tox_60</th>\n",
       "      <th>tox_70</th>\n",
       "      <th>tox_80</th>\n",
       "      <th>tox_90</th>\n",
       "      <th>insult_60</th>\n",
       "      <th>insult_70</th>\n",
       "      <th>insult_80</th>\n",
       "      <th>insult_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119343</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122343</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_obs  coder_1  coder_2  consensus  sec_create_1  sec_create_2  \\\n",
       "0  101238        0        0        1.0            46            28   \n",
       "1  119343        0        0        1.0             8             6   \n",
       "2  122343        0        0        1.0             8             6   \n",
       "3  131878        0        0        1.0             4            52   \n",
       "4  132171        0        0        1.0             6            15   \n",
       "\n",
       "   sec_review_1  sec_review_2  possibly_sensitive lang  ...  THREAT  \\\n",
       "0            17             8               False   es  ...     NaN   \n",
       "1             0             2               False   es  ...     NaN   \n",
       "2             1             0               False   es  ...     NaN   \n",
       "3             0             1               False   es  ...     NaN   \n",
       "4             0             1               False   es  ...     NaN   \n",
       "\n",
       "         date tox_60  tox_70  tox_80  tox_90  insult_60  insult_70  insult_80  \\\n",
       "0  2020-08-17      0       0       0       0          0          0          0   \n",
       "1  2020-08-17      0       0       0       0          0          0          0   \n",
       "2  2020-08-17      0       0       0       0          0          0          0   \n",
       "3  2020-08-17      0       0       0       0          0          0          0   \n",
       "4  2020-08-17      0       0       0       0          0          0          0   \n",
       "\n",
       "   insult_90  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Load CSV\n",
    "df = pd.read_csv(\"hf://datasets/bgonzalezbustamante/toxicity-protests-ES/goldstd_protests.csv\")\n",
    "\n",
    "## Recoding\n",
    "## df = df[['coder_1','text']]\n",
    "## df['coder_1'] = df['coder_1'].map({0: 'NONTOXIC', 1: 'TOXIC'})\n",
    "\n",
    "## Check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc3a41-9eb9-4da1-909c-e81375dc18be",
   "metadata": {},
   "source": [
    "## Ollama straightforward classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e73e3-1a08-44e7-83e8-b81625b15944",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification\n",
    "**Simple yet effective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2287ff-ac85-4fd9-b094-76b8bd514f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 1000/1000 [02:23<00:00,  6.99it/s]\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Model\n",
    "model = \"llama3.2\" ## 3B (2GB)\n",
    "\n",
    "## Store classification results\n",
    "df[\"annotation\"] = \"\" \n",
    "\n",
    "## Zero-shot with a progress bar\n",
    "for i in tqdm(range(len(df)), desc=\"Classifying comments\"):\n",
    "    chat_response = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"seed\":86,\"temperature\":0,\"top_p\":0.9,\"repeat_penalty\":1.1,\"top_k\":40,\"min_p\":0,\"num_ctx\":2048},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Classify the category of the comment as either TOXIC or NONTOXIC. TOXIC: Rude, disrespectful, or unreasonable comments that are likely to make someone leave the discussion or stop sharing their perspective. NONTOXIC: Civil or nice comments that are unlikely to discourage conversation.\" \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"text: {df.loc[i, 'text']} \"\n",
    "                    \"\\nRespond with only the category (TOXIC or NONTOXIC). Do not provide any additional analysis or explanation.\"\n",
    "                ),\n",
    "                },\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    ## Store the result in the 'classification' column\n",
    "    df.at[i, \"annotation\"] = chat_response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8ce3e7-20d9-4ed1-92c6-7b73d2a19c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n",
      "TOXIC                                                                                                     734\n",
      "NONTOXIC                                                                                                  265\n",
      "I cannot classify this comment as it contains hate speech. Is there anything else I can help you with?      1\n",
      "Name: count, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Summary\n",
    "print(df[\"annotation\"].value_counts())\n",
    "print(df[\"annotation\"].isna().sum())\n",
    "unique_values = df[\"annotation\"].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34db518-e8f3-4bdc-bd2f-d5f5503a2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n",
      "1    734\n",
      "0    266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Mapping labels\n",
    "mapping = {unique_values[0]: 1, unique_values[1]: 0, unique_values[2]: 0}\n",
    "df['annotation'] = df['annotation'].map(mapping)\n",
    "print(df[\"annotation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4432f924-e1e0-443a-a82c-6db814d12030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.726 Precision: 0.6771117166212534 Recall 0.9307116104868914 F1-Score 0.7839116719242902\n"
     ]
    }
   ],
   "source": [
    "## Performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(df[\"coder_1\"], df[\"annotation\"])\n",
    "precision = precision_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "recall = recall_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "f1 = f1_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "print(\"Accuracy:\", accuracy, \"Precision:\", precision, \"Recall\", recall, \"F1-Score\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f139fe-c4fe-4de0-94b9-b98a13ebd6c8",
   "metadata": {},
   "source": [
    "### Few-Shot Classification\n",
    "**More complex, but requires good examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6f1876-73b5-4b09-8a45-655e819b0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 1000/1000 [02:22<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Model\n",
    "model = \"llama3.2\"\n",
    "\n",
    "## Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Classify the category of the comment as either TOXIC or NONTOXIC. TOXIC: Rude, disrespectful, or unreasonable comments that are likely to make someone leave the discussion or stop sharing their perspective. NONTOXIC: Civil or nice comments that are unlikely to discourage conversation.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"text: 'Eres un idiota.'\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"TOXIC\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"text: 'Muchas gracias por tu interesante comentario.'\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"NONTOXIC\"\n",
    "    }\n",
    "]\n",
    "\n",
    "## Store classification results\n",
    "df[\"annotation\"] = \"\"\n",
    "\n",
    "## Few-shot classification with a progress bar\n",
    "for i in tqdm(range(len(df)), desc=\"Classifying comments\"):\n",
    "    chat_response = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"seed\": 86, \"temperature\": 0, \"top_p\": 0.9, \"repeat_penalty\": 1.1, \"top_k\": 40, \"min_p\": 0, \"num_ctx\": 2048},\n",
    "        messages=few_shot_examples + [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"text: {df.loc[i, 'text']} \"\n",
    "                    \"\\nRespond with only the category (TOXIC or NONTOXIC). Do not provide any additional analysis or explanation.\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    ## Store the result in the 'classification' column\n",
    "    df.at[i, \"annotation\"] = chat_response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa79f66b-cda0-43e9-b1b9-025f599ad92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n",
      "TOXIC       884\n",
      "NONTOXIC    116\n",
      "Name: count, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Summary\n",
    "print(df[\"annotation\"].value_counts())\n",
    "print(df[\"annotation\"].isna().sum())\n",
    "unique_values = df[\"annotation\"].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171335a5-25b2-4a4e-9472-69beba192ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n",
      "1    884\n",
      "0    116\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Mapping labels\n",
    "mapping = {unique_values[0]: 1, unique_values[1]: 0}\n",
    "df['annotation'] = df['annotation'].map(mapping)\n",
    "print(df[\"annotation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9973e6-ab16-434d-b017-cad91de814c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.634 Precision: 0.5950226244343891 Recall 0.9850187265917603 F1-Score 0.7418899858956276\n"
     ]
    }
   ],
   "source": [
    "## Performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(df[\"coder_1\"], df[\"annotation\"])\n",
    "precision = precision_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "recall = recall_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "f1 = f1_score(df[\"coder_1\"], df[\"annotation\"], average=\"binary\")\n",
    "print(\"Accuracy:\", accuracy, \"Precision:\", precision, \"Recall\", recall, \"F1-Score\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881eb3d-5cd3-4e8b-b325-e37e7d795ad5",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Classification\n",
    "**It is time-consuming and does not always offer improvements; it depends significantly on the model (better performance in larger models) and the labelling structure. JSON should give better results** \\\n",
    "**Some models might not strictly adhere to the prompt instructions due to their training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "397800e0-7afa-44c1-9884-70b84ba207a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 1000/1000 [21:12<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "## Model\n",
    "model = \"llama3.2\"\n",
    "\n",
    "## Store classification results\n",
    "df[\"annotation\"] = \"\"\n",
    "df[\"reasoning\"] = \"\"\n",
    "\n",
    "## Chain-of-Thought classification with a progress bar\n",
    "for i in tqdm(range(len(df)), desc=\"Classifying comments\"):\n",
    "    chat_response = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"seed\": 86, \"temperature\": 0, \"top_p\": 0.9, \"repeat_penalty\": 1.1, \"top_k\": 40, \"min_p\": 0, \"num_ctx\": 2048},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Classify the category of the comment as either TOXIC or NONTOXIC. TOXIC: Rude, disrespectful, or unreasonable comments that are likely to make someone leave the discussion or stop sharing their perspective. NONTOXIC: Civil or nice comments that are unlikely to discourage conversation. \"\n",
    "                     \"First, provide your reasoning step-by-step. On a new line, explicitly state the category in the exact format: 'Label: TOXIC' or 'Label: NONTOXIC'. Do not include any additional text after the label.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"text: {df.loc[i, 'text']} \"\n",
    "                    \"\\nFirst, explain your reasoning step-by-step. Then, conclude with the label on a new line in the exact format: 'Label: TOXIC' or 'Label: NONTOXIC'.\"\n",
    "                )\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    ## Parse the response\n",
    "    response_content = chat_response['message']['content']\n",
    "    label_match = re.search(r\"Label:\\s*(TOXIC|NONTOXIC)\", response_content, re.IGNORECASE)\n",
    "    if label_match:\n",
    "        final_label = label_match.group(1).upper()\n",
    "        reasoning = response_content[:label_match.start()].strip()\n",
    "    else:\n",
    "        final_label = \"INVALID\"\n",
    "        reasoning = response_content.strip()\n",
    "\n",
    "    ## Store the results\n",
    "    df.at[i, \"reasoning\"] = reasoning\n",
    "    df.at[i, \"annotation\"] = final_label if final_label in [\"TOXIC\", \"NONTOXIC\"] else \"INVALID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89880ebc-f7fc-4844-9a8e-eeedec3c143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n",
      "TOXIC       735\n",
      "NONTOXIC    218\n",
      "INVALID      47\n",
      "Name: count, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Summary\n",
    "print(df[\"annotation\"].value_counts())\n",
    "print(df[\"annotation\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586392c4-e2a6-47c1-b42f-e3c1a6624d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_annotation\n",
      "TOXIC            743\n",
      "NONTOXIC         241\n",
      "REVIEW_NEEDED     16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Post-process reasoning for reclassification\n",
    "def reclassify_from_reasoning(reasoning):\n",
    "    reasoning_lower = reasoning.lower()\n",
    "    if 'toxic' in reasoning_lower and 'non' not in reasoning_lower:\n",
    "        return 'TOXIC'\n",
    "    elif 'non-toxic' in reasoning_lower or 'not toxic' in reasoning_lower or 'nontoxic' in reasoning_lower:\n",
    "        return 'NONTOXIC'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "## Create 'final_annotation'\n",
    "df['final_annotation'] = df['annotation']\n",
    "\n",
    "## Identify INVALID rows\n",
    "invalid_mask = df['annotation'] == 'INVALID'\n",
    "\n",
    "## Apply the reclassification function\n",
    "df.loc[invalid_mask, 'final_annotation'] = df.loc[invalid_mask, 'reasoning'].apply(reclassify_from_reasoning)\n",
    "\n",
    "## Labels for manual review\n",
    "unknown_mask = df['final_annotation'] == 'UNKNOWN'\n",
    "df.loc[unknown_mask, 'final_annotation'] = 'REVIEW_NEEDED'\n",
    "\n",
    "## Values for mapping\n",
    "unique_values = df[\"final_annotation\"].value_counts().index.tolist()\n",
    "print(df[\"final_annotation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad6ac49-df36-4892-9a47-630a156f30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_annotation\n",
      "1    743\n",
      "0    257\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Mapping labels\n",
    "mapping = {unique_values[0]: 1, unique_values[1]: 0, unique_values[2]: 0}\n",
    "df['final_annotation'] = df['final_annotation'].map(mapping)\n",
    "print(df[\"final_annotation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f199ff48-64bd-49a7-9905-4c2e49ec5b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.649 Precision: 0.6231493943472409 Recall 0.8670411985018727 F1-Score 0.7251370399373531\n"
     ]
    }
   ],
   "source": [
    "## Performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(df[\"coder_1\"], df[\"final_annotation\"])\n",
    "precision = precision_score(df[\"coder_1\"], df[\"final_annotation\"], average=\"binary\")\n",
    "recall = recall_score(df[\"coder_1\"], df[\"final_annotation\"], average=\"binary\")\n",
    "f1 = f1_score(df[\"coder_1\"], df[\"final_annotation\"], average=\"binary\")\n",
    "print(\"Accuracy:\", accuracy, \"Precision:\", precision, \"Recall\", recall, \"F1-Score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb467d42-9336-4ce9-8dd5-77944ce901de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
